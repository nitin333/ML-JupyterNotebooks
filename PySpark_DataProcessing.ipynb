{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a20c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apache Spark is written in Scala programming language. To support Python with Spark, Apache Spark community released a tool,\n",
    "PySpark. Using PySpark, you can work with RDDs in Python programming language also. It is because of a library called Py4j \n",
    "that they are able to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717dba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apache Spark is a lightning fast real-time processing framework. It does in-memory computations to analyze data in real-time.\n",
    "PySpark offers PySpark Shell which links the Python API to the spark core and initializes the Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, \n",
    "which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside\n",
    "the executors on worker nodes.\n",
    "\n",
    "SparkContext uses Py4J to launch a JVM and creates a JavaSparkContext. By default, PySpark has SparkContext available as\n",
    "‘sc’, so creating a new SparkContext won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befad111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark In-memory Computing\n",
    "\n",
    "1. Keeping the data in-memory improves the performance by an order of magnitudes. The main abstraction of Spark is its RDDs.\n",
    "   And the RDDs are cached using the cache() or persist() method.\n",
    "    \n",
    "2. The in-memory capability of Spark is good for machine learning and micro-batch processing. It provides faster execution \n",
    "   for iterative jobs.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b17e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy evaluation\n",
    "\n",
    "1. lazy evaluation in Spark means that the execution will not start until an action is triggered. In Spark, the picture \n",
    "   of lazy evaluation comes when Spark transformations occur\n",
    "    \n",
    "2. Transformations are lazy in nature meaning when we call some operation in RDD, it does not execute immediately. Spark \n",
    "   maintains the record of which operation is being called(Through DAG). We can think Spark RDD as the data, that we built \n",
    "   up through transformation. Since transformations are lazy in nature, so we can execute operation any time by calling an \n",
    "   action on data. Hence, in lazy evaluation data is not loaded until it is necessary.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ce65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fault tolerance \n",
    "\n",
    "1. Since Apache Spark RDD is an immutable dataset, each Spark RDD remembers the lineage of the deterministic operation \n",
    "   that was used on fault-tolerant input dataset to create it.\n",
    "\n",
    "2. If due to a worker node failure any partition of an RDD is lost, then that partition can be re-computed from the \n",
    "   original fault-tolerant dataset using the lineage of operations.\n",
    "\n",
    "3. Assuming that all of the RDD transformations are deterministic, the data in the final transformed RDD will always be \n",
    "   the same irrespective of failures in the Spark cluster.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformations & Actions\n",
    "\n",
    "#Narrow Transformations (data will reside on single cluster)\n",
    "\n",
    "1. Map 2. FlatMap 3. MapPartition 4.Filter 5.Sample 6.Union\n",
    "\n",
    "# Wide Transformations (Data will reside on different cluster)\n",
    "\n",
    "1. Intersection 2. Distinct 3. ReduceByKey 4. GroupByKey 5. Join 6. Cartesian 7. Repartition 8. Coalesce\n",
    "\n",
    "#Action2\n",
    "\n",
    "1. count 2. first 3. take,takeSample 4. reduce 5. collect 6. top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfa5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415072c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.rdd import RDD\n",
    "conf = SparkConf().setAppName(\"Collinear Points\")\n",
    "sc = SparkContext('local',conf=conf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aafcc870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c831b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = sc.parallelize(dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1798168",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"PATH/blogtexts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783163e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(5)\n",
    "# Output:\n",
    "[u'Think of it for a moment \\u2013 1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.',\n",
    " u'',\n",
    " u'Big Data is not a new phenomena. It has been around for a while now. However, it has become really important with this pace of data generation. In past, several systems were developed for processing big data. Most of them were based on MapReduce framework. These frameworks typically rely on use of hard disk for saving and retrieving the results. However, this turns out to be very costly in terms of time and speed.',\n",
    " u'',\n",
    " u'On the other hand, Organizations have never been more hungrier to add a competitive differentiation through understanding this data and offering its customer a much better experience. Imagine how valuable would be Facebook, if it did not understand your interests well? The traditional hard disk based MapReduce kind of frameworks do not help much to address this challenge.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a1f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "def Func(lines):\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split()\n",
    "    return lines\n",
    "rdd1 = rdd.map(Func)\n",
    "# Output: [[u'think', u'of', u'it', u'for', u'a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28168ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat Map\n",
    "# The “flatMap” transformation will return a new RDD by first applying a function to all elements of this RDD, and then\n",
    "# flattening the results\n",
    "\n",
    "rdd2 = rdd.flatMap(Func)\n",
    "rdd2.take(5)\n",
    "\n",
    "# Output: [u'think', u'of', u'it', u'for', u'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc1a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation: filter\n",
    "stopwords = ['is','am','are','the','for','a']\n",
    "rdd3 = rdd2.filter(lambda x: x not in stopwords)\n",
    "rdd3.take(10)\n",
    "Output:[u'think',u'of',u'it',u'moment',u'\\u2013',u'1',u'qunitillion',u'=',u'1',u'million']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation: groupBy\n",
    "rdd4 = rdd3.groupBy(lambda w: w[0:3])\n",
    "print [(k, list(v)) for (k, v) in rdd4.take(1)]\n",
    "Output: [(u'all', [u'all', u'allocates', u'all', u'all', u'allows', u'all', u'all', u'all', u'all', u'all', u'all', u'all'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation: groupByKey / reduceByKey \n",
    "rdd3_mapped = rdd3.map(lambda x: (x,1))\n",
    "rdd3_grouped = rdd3_mapped.groupByKey()\n",
    "print(list((j[0], list(j[1])) for j in rdd3_grouped.take(5)))\n",
    "Output: [(u'all', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), (u'elements,', [1, 1]), (u'step2:', [1]), (u'manager', [1]), (u'(if', [1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3_freq_of_words = rdd3_grouped.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)\n",
    "rdd3_freq_of_words.take(10)\n",
    "output:\n",
    "[(164, u'to'),\n",
    " (143, u'in'),\n",
    " (122, u'of'),\n",
    " (106, u'and'),\n",
    " (103, u'we'),\n",
    " (69, u'spark'),\n",
    " (64, u'this'),\n",
    " (63, u'data'),\n",
    " (55, u'can'),\n",
    " (52, u'apache')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b122e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3_mapped.reduceByKey(lambda x,y: x+y).map(lambda x:(x[1],x[0])).sortByKey(False).take(10)\n",
    "output:\n",
    "[(164, u'to'),\n",
    " (143, u'in'),\n",
    " (122, u'of'),\n",
    " (106, u'and'),\n",
    " (103, u'we'),\n",
    " (69, u'spark'),\n",
    " (64, u'this'),\n",
    " (63, u'data'),\n",
    " (55, u'can'),\n",
    " (52, u'apache')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fcadd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Project\",\n",
    "\"Gutenberg’s\",\n",
    "\"Alice’s\",\n",
    "\"Adventures\",\n",
    "\"in\",\n",
    "\"Wonderland\",\n",
    "\"Project\",\n",
    "\"Gutenberg’s\",\n",
    "\"Adventures\",\n",
    "\"in\",\n",
    "\"Wonderland\",\n",
    "\"Project\",\n",
    "\"Gutenberg’s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "724747ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05b43490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Alice’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n"
     ]
    }
   ],
   "source": [
    "rdd2=rdd.map(lambda x: (x,1))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f25e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [('James','Smith','M',30),\n",
    "  ('Anna','Rose','F',41),\n",
    "  ('Robert','Williams','M',62), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8c1c4b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|    30|\n",
      "|     Anna|    Rose|     F|    41|\n",
      "|   Robert|Williams|     M|    62|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data1, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e06efc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     M|        60|\n",
      "|      Anna,Rose|     F|        82|\n",
      "|Robert,Williams|     M|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd2=df.rdd.map(lambda x:(x[0]+\",\"+x[1],x[2],x[3]*2))  \n",
    "df2=rdd2.toDF([\"name\",\"gender\",\"new_salary\"]   )\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "954b1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws,col,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a29cf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     M|        60|\n",
      "|      Anna,Rose|     F|        82|\n",
      "|Robert,Williams|     M|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(concat_ws(\",\",df.firstname,df.lastname).alias(\"name\"), \\\n",
    "          df.gender,lit(df.salary*2).alias(\"new_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ae79f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d22872d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James M\n",
      "Anna F\n",
      "Robert M\n"
     ]
    }
   ],
   "source": [
    "pandasDF = df.toPandas()\n",
    "for index, row in pandasDF.iterrows():\n",
    "    print(row['firstname'], row['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157e5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "701e2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1022f4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[1] appName=SparkByExamples.com>\n",
      "Spark App Name : SparkByExamples.com\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "print(spark.sparkContext)\n",
    "print(\"Spark App Name : \"+ spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b95580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data\n",
    "\n",
    "csv_df = spark.read.csv(\"......./my_buckets/data.csv\")\n",
    "\n",
    "# Reading Header\n",
    "csv_df = spark.read.csv(\"....../my_buckets/poland_ks\", header = \"true\")\n",
    "\n",
    "# Print schema\n",
    "csv_df.printSchema()\n",
    "\n",
    "# Infer schema while reading data\n",
    "csv_df = spark.read.csv(\"....../my_buckets/data.csv\", header =True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9608481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Jason\n",
    "jason_df = spark.read.json(\"....../data.json\")\n",
    "\n",
    "# Reading Parquet\n",
    "parquet_df = spark.read.parquet(\"....../data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907828cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"CLIENTNUM\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"superior_emp_id\", StringType()),\n",
    "    StructField(\"year_joined\", DateType()),\n",
    "    StructField(\"emp_dept_id\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"salary\", IntegerType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c404759",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('churn-cc').getOrCreate()\n",
    "df = spark.read.csv('credit_card_churn.csv', header = True, schema = custom_schema,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dcc491f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CLIENTNUM|Attrition_Flag   |Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1|Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|768805383|Existing Customer|45          |M     |3              |High School    |Married       |$60K - $80K    |Blue         |39            |5                       |1                     |3                    |12691.0     |777                |11914.0        |1.335               |1144           |42            |1.625              |0.061                |9.34E-5                                                                                                                           |0.99991                                                                                                                           |\n",
      "|818770008|Existing Customer|49          |F     |5              |Graduate       |Single        |Less than $40K |Blue         |44            |6                       |1                     |2                    |8256.0      |864                |7392.0         |1.541               |1291           |33            |3.714              |0.105                |5.69E-5                                                                                                                           |0.99994                                                                                                                           |\n",
      "|713982108|Existing Customer|51          |M     |3              |Graduate       |Married       |$80K - $120K   |Blue         |36            |4                       |1                     |0                    |3418.0      |0                  |3418.0         |2.594               |1887           |20            |2.333              |0.0                  |2.11E-5                                                                                                                           |0.99998                                                                                                                           |\n",
      "|769911858|Existing Customer|40          |F     |4              |High School    |Unknown       |Less than $40K |Blue         |34            |3                       |4                     |1                    |3313.0      |2517               |796.0          |1.405               |1171           |20            |2.333              |0.76                 |1.3366E-4                                                                                                                         |0.99987                                                                                                                           |\n",
      "|709106358|Existing Customer|40          |M     |3              |Uneducated     |Married       |$60K - $80K    |Blue         |21            |5                       |1                     |0                    |4716.0      |0                  |4716.0         |2.175               |816            |28            |2.5                |0.0                  |2.17E-5                                                                                                                           |0.99998                                                                                                                           |\n",
      "|713061558|Existing Customer|44          |M     |2              |Graduate       |Married       |$40K - $60K    |Blue         |36            |3                       |1                     |2                    |4010.0      |1247               |2763.0         |1.376               |1088           |24            |0.846              |0.311                |5.51E-5                                                                                                                           |0.99994                                                                                                                           |\n",
      "|810347208|Existing Customer|51          |M     |4              |Unknown        |Married       |$120K +        |Gold         |46            |6                       |1                     |3                    |34516.0     |2264               |32252.0        |1.975               |1330           |31            |0.722              |0.066                |1.2303E-4                                                                                                                         |0.99988                                                                                                                           |\n",
      "|818906208|Existing Customer|32          |M     |0              |High School    |Unknown       |$60K - $80K    |Silver       |27            |2                       |2                     |2                    |29081.0     |1396               |27685.0        |2.204               |1538           |36            |0.714              |0.048                |8.58E-5                                                                                                                           |0.99991                                                                                                                           |\n",
      "|710930508|Existing Customer|37          |M     |3              |Uneducated     |Single        |$60K - $80K    |Blue         |36            |5                       |2                     |0                    |22352.0     |2517               |19835.0        |3.355               |1350           |24            |1.182              |0.113                |4.48E-5                                                                                                                           |0.99996                                                                                                                           |\n",
      "|719661558|Existing Customer|48          |M     |2              |Graduate       |Single        |$80K - $120K   |Blue         |36            |6                       |3                     |3                    |11656.0     |1677               |9979.0         |1.524               |1441           |32            |0.882              |0.144                |3.0251E-4                                                                                                                         |0.9997                                                                                                                            |\n",
      "|708790833|Existing Customer|42          |M     |5              |Uneducated     |Unknown       |$120K +        |Blue         |31            |5                       |3                     |2                    |6748.0      |1467               |5281.0         |0.831               |1201           |42            |0.68               |0.217                |1.9094E-4                                                                                                                         |0.99981                                                                                                                           |\n",
      "|710821833|Existing Customer|65          |M     |1              |Unknown        |Married       |$40K - $60K    |Blue         |54            |6                       |2                     |3                    |9095.0      |1587               |7508.0         |1.433               |1314           |26            |1.364              |0.174                |1.9751E-4                                                                                                                         |0.9998                                                                                                                            |\n",
      "|710599683|Existing Customer|56          |M     |1              |College        |Single        |$80K - $120K   |Blue         |36            |3                       |6                     |0                    |11751.0     |0                  |11751.0        |3.397               |1539           |17            |3.25               |0.0                  |4.78E-5                                                                                                                           |0.99995                                                                                                                           |\n",
      "|816082233|Existing Customer|35          |M     |3              |Graduate       |Unknown       |$60K - $80K    |Blue         |30            |5                       |1                     |3                    |8547.0      |1666               |6881.0         |1.163               |1311           |33            |2.0                |0.195                |9.61E-5                                                                                                                           |0.9999                                                                                                                            |\n",
      "|712396908|Existing Customer|57          |F     |2              |Graduate       |Married       |Less than $40K |Blue         |48            |5                       |2                     |2                    |2436.0      |680                |1756.0         |1.19                |1570           |29            |0.611              |0.279                |1.1382E-4                                                                                                                         |0.99989                                                                                                                           |\n",
      "|714885258|Existing Customer|44          |M     |4              |Unknown        |Unknown       |$80K - $120K   |Blue         |37            |5                       |1                     |2                    |4234.0      |972                |3262.0         |1.707               |1348           |27            |1.7                |0.23                 |6.35E-5                                                                                                                           |0.99994                                                                                                                           |\n",
      "|709967358|Existing Customer|48          |M     |4              |Post-Graduate  |Single        |$80K - $120K   |Blue         |36            |6                       |2                     |3                    |30367.0     |2362               |28005.0        |1.708               |1671           |27            |0.929              |0.078                |2.3623E-4                                                                                                                         |0.99976                                                                                                                           |\n",
      "|753327333|Existing Customer|41          |M     |3              |Unknown        |Married       |$80K - $120K   |Blue         |34            |4                       |4                     |1                    |13535.0     |1291               |12244.0        |0.653               |1028           |21            |1.625              |0.095                |1.4953E-4                                                                                                                         |0.99985                                                                                                                           |\n",
      "|806160108|Existing Customer|61          |M     |1              |High School    |Married       |$40K - $60K    |Blue         |56            |2                       |2                     |3                    |3193.0      |2517               |676.0          |1.831               |1336           |30            |1.143              |0.788                |1.7468E-4                                                                                                                         |0.99983                                                                                                                           |\n",
      "|709327383|Existing Customer|45          |F     |2              |Graduate       |Married       |Unknown        |Blue         |37            |6                       |1                     |2                    |14470.0     |1157               |13313.0        |0.966               |1207           |21            |0.909              |0.08                 |5.51E-5                                                                                                                           |0.99994                                                                                                                           |\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "197aad02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CLIENTNUM',\n",
       " 'Attrition_Flag',\n",
       " 'Customer_Age',\n",
       " 'Gender',\n",
       " 'Dependent_count',\n",
       " 'Education_Level',\n",
       " 'Marital_Status',\n",
       " 'Income_Category',\n",
       " 'Card_Category',\n",
       " 'Months_on_book',\n",
       " 'Total_Relationship_Count',\n",
       " 'Months_Inactive_12_mon',\n",
       " 'Contacts_Count_12_mon',\n",
       " 'Credit_Limit',\n",
       " 'Total_Revolving_Bal',\n",
       " 'Avg_Open_To_Buy',\n",
       " 'Total_Amt_Chng_Q4_Q1',\n",
       " 'Total_Trans_Amt',\n",
       " 'Total_Trans_Ct',\n",
       " 'Total_Ct_Chng_Q4_Q1',\n",
       " 'Avg_Utilization_Ratio',\n",
       " 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n",
       " 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4917124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.where(col(\"Education_Level\") ==\"Graduate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6616bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3128"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa76527",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Persist\n",
    "\n",
    "1. Using persist() method, PySpark provides an optimization mechanism to store the intermediate computation of a PySpark \n",
    "DataFrame so they can be reused in subsequent actions.\n",
    "\n",
    "2. When we persist a dataset, each node stores its partitioned data in memory and reuses them in other actions on that \n",
    "dataset. And PySpark persisted data on nodes are fault-tolerant meaning if any partition of a Dataset is lost, it will \n",
    "automatically be recomputed using the original transformations that created it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10224840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the DataFrame\n",
    "df1 = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe45eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting csv \n",
    "# By using coalesce(1) or repartition(1) all the partitions of the dataframe are combined in a single block.\n",
    "\n",
    "partitioned_output.coalesce(1).write.mode(\"overwrite\")\\\n",
    ".format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"sep\", \"|\")\\\n",
    ".save('......../my_output_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae48531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting column from df\n",
    "df1 = df.select('c1','c2','c3','c4')\n",
    "df1.show()\n",
    "\n",
    "# Renaming column\n",
    "df1 = df.withColumnRenamed(\"c1\",\"renamed_c1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08e5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the schema\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "cases = cases.withColumn('c1', F.col('c1').cast(IntegerType()))\n",
    "cases = cases.withColumn('c2', F.col('c2').cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER\n",
    "df.filter((df.c1>10) & (df.c2=='AAB')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae489c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting\n",
    "df.sort(\"c1\").show()\n",
    "\n",
    "# descending Sort\n",
    "from pyspark.sql import functions as F\n",
    "df.sort(F.desc(\"c1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy\n",
    "from pyspark.sql import functions as F\n",
    "df1 = df.groupBy([\"c1\",\"c2\"]).agg(F.sum(\"c3\"),F.max(\"c4\"))\n",
    "\n",
    "# Use alias to rename columns\n",
    "df1 = df.groupBy([\"c1\",\"c2\"]).agg(\n",
    "    F.sum(\"c3\").alias(\"c3_renamed\"),\\\n",
    "    F.max(\"c4\").alias(\"c4_renamed\")\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304d060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping duplicates\n",
    "df1 = df.dropDuplicates([\"department\",\"salary\"])\n",
    "df.withColumn('duplicated', F.count('*').over(W.partitionBy('ncf', 'date').orderBy(F.lit(1))) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db03aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### deciles\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df1 = df1.select(\"Item_group\",\"Item_name\",\"Price\", F.ntile(10).over(Window.partitionBy().orderBy(df1['price'])).alias(\"decile_rank\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05438c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins1\n",
    "df3 = df1.join(df2, ['province','city'],how='left')\n",
    "df3.limit(10)\n",
    "\n",
    "# Joins2\n",
    "df1.join(df2,[col(f) == col(s) for (f, s) in zip(df1,df2)], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2,(df1[\"col1\"]==df2[\"col2\"]) & (df1[\"col3\"]==df2[\"col4\"]),how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "374d6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"emp_id\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"superior_emp_id\", StringType()),\n",
    "    StructField(\"year_joined\", DateType()),\n",
    "    StructField(\"emp_dept_id\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"salary\", IntegerType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ee65392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0247fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e74faac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ff1ed71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast\n",
    "#Sometimes, we might face a scenario in which we need to join a very big table (~1B rows) with a very \n",
    "#small table (~100–200 rows).\n",
    "\n",
    "#PySpark Broadcast Join is an important part of the SQL execution engine, With broadcast join, PySpark broadcast the smaller\n",
    "#DataFrame to all executors and the executor keeps this DataFrame in memory and the larger DataFrame is split and \n",
    "#distributed across all executors so that PySpark can perform a join without shuffling any data from the larger DataFrame as\n",
    "#the data required for join colocated on every executor.\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "cases = big_df.join(broadcast(small_df), ['province','city'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e26668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = largeDF.join(broadcast(smallerDF),smallerDF(\"code\")  largeDF(\"UNIT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daffadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQL With Data Frames\n",
    "df.registerTempTable('df_table')\n",
    "new_df = sqlContext.sql('select * from df_table where c1 > 100')\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34b1d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+----+\n",
      "|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|rank|\n",
      "+------+-----+---------------+-----------+-----------+------+------+----+\n",
      "|     1|Smith|             -1|       2018|         10|     M|  3000|   1|\n",
      "|     4|Jones|              2|       2005|         10|     F|  2000|   2|\n",
      "|     2| Rose|              1|       2010|         20|     M|  4000|   1|\n",
      "|     5|Brown|              2|       2010|         40|      |    -1|   1|\n",
      "|     6|Brown|              2|       2010|         50|      |    -1|   1|\n",
      "+------+-----+---------------+-----------+-----------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark Window Functions\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "windowSpec = Window().partitionBy(['emp_dept_id']).orderBy(F.desc('salary'))\n",
    "empDF.withColumn(\"rank\",F.rank().over(windowSpec)).filter(col(\"rank\") <= 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edab34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a621657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+-----+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|bonus|\n",
      "+------+--------+---------------+-----------+-----------+------+------+-----+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|60000|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|80000|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|20000|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|40000|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|  -20|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|  -20|\n",
      "+------+--------+---------------+-----------+-----------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new column\n",
    "import pyspark.sql.functions as F\n",
    "empDF1 = empDF.withColumn(\"bonus\", 20 * F.col(\"salary\"))\n",
    "empDF1.show()\n",
    "\n",
    "# exponential\n",
    "new_df = df.withColumn(\"exp_c1\", F.exp(\"c1\"))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d9dbee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+-----+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|bonus|\n",
      "+------+--------+---------------+-----------+-----------+------+------+-----+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|60000|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|80000|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|20000|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|40000|\n",
      "|     5|   Brown|              2|       2010|         40|  null|    -1|  -20|\n",
      "|     6|   Brown|              2|       2010|         50|  null|    -1|  -20|\n",
      "+------+--------+---------------+-----------+-----------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,when\n",
    "empDF2=empDF1.select([when(col(c)==\"\",None).otherwise(col(c)).alias(c) for c in empDF1.columns])\n",
    "empDF2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a65117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "def HighLow(c1):\n",
    "    if c1 < 50: \n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'high'\n",
    "    \n",
    "#convert to a UDF Function by passing in the function and return type of function\n",
    "HighLowUDF = F.udf(HighLow, StringType())\n",
    "df1 = df.withColumn(\"HighLow\", HighLowUDF(\"c1\"))\n",
    "df1.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
